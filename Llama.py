import torch
from torch import nn
from torch.nn import functional as F
import numpy as np
from matplotlib import pyplot as plt
import time
import pandas as pd

#1 million characters dataset


lines=open('input.txt','r').read()


vocab=sorted(list(set(lines)))

itos={i:ch for i , ch in enumerate(vocab)}
stoi={ch:i for i,ch in enumerate(vocab)}
#print(lines[:1000])


#tokenization by chars

def encode(s):
    return [stoi[ch] for ch in s ]
def decode(l):
    return ''.join([itos[i] for i in l])


print('vocab size :' ,len(vocab))
print(decode(encode("Ranjan")))


#using the master configs

MASTER_CONFIG={
    "vocab_size":len(vocab),
}

dataset=torch.tensor(encode(lines),dtype=torch.int8)

print(dataset.shape)


def get_batches(data,split,batch_size,context_window,config=MASTER_CONFIG):
    train=data[:int(.8*len(data))]
    val=data[int(.8*len(data)):int(.9*len(data))]

    test=data[int(.9*len(data)):]


    batch_data=train
    if split=='train':
        batch_data=val
    if split=='test':
        batch_data=test

    ix=torch.randint(0,batch_data.size(0)-context_window-1,(batch_size,))
    x=torch.stack([batch_data[i:i+context_window] for i in ix ]).long()

    y=torch.stack([batch_data[i+1:i+context_window+1] for i in ix]).long()
    return x,y


MASTER_CONFIG.update({
    'batch_size':8,
    'context_window':16
})

xs,ys=get_batches(dataset,'train',MASTER_CONFIG['batch_size'],MASTER_CONFIG['context_window'])

print([(decode(xs[i].tolist()), decode(ys[i].tolist())) for i in range(len(xs))])


@torch.no_grad()

def evaluate_loss(model,config=MASTER_CONFIG):
    out={}
    model.eval()
    for split in ["train","val"]:
        losses=[]
        for _ in range(10):
            xb,yb=get_batches(dataset,split,config['batch_size'],config['context_window'])
            _,loss=model(xb,yb)
            losses.append(loss.item())
        out[split]=np.mean(losses)
    model.train()
    return out
class SimpleBrokenModel(nn.Module):
    def __init__(self,config=MASTER_CONFIG):
        super().__init__()
        self.config=config

        self.embedding=nn.Embedding(config['vocab_size'],config['d_model'])
        self.linear=nn.Sequential(
            nn.Linear(config['d_model'],config['d_model']),
            nn.ReLU(),
            nn.Linear(config['d_model'],config['vocab_size']),


        )
        print("model params=====> ",sum([m.numel() for m in self.parameters()]))
    def forward(self,idx,targets=None):
        x=self.embedding(idx)
        #x=x.view(-1,self.config['d_model'])
        a=self.linear(x)

        logits=F.softmax(a,dim=-1)

        if targets is not None:
            loss=F.cross_entropy(logits.view(-1,self.config['vocab_size']),targets.view(-1))
            return logits,loss
        else:
            return logits
MASTER_CONFIG.update({
    'd_model':128,
})

model=SimpleBrokenModel(MASTER_CONFIG)

xs,ys=get_batches(dataset,'train',MASTER_CONFIG['batch_size'],MASTER_CONFIG['context_window'])
logits,loss=model(xs,ys)



MASTER_CONFIG.update({
    'epochs':1000,
    'log_interval':10,
    'batch_size':32,
})

model=SimpleBrokenModel(MASTER_CONFIG)

optimizer=torch.optim.Adam(
    model.parameters(),
)

def train(model,optimizer,scheduler=None,config=MASTER_CONFIG,print_logs=True):
    losses=[]
    start_time=time.time()
    for epoch in range(config['epochs']):
        optimizer.zero_grad()

        xs,ys=get_batches(dataset,'train',config['batch_size'],config['context_window'])
        logits,loss=model(xs,targets=ys)
        loss.backward()

        optimizer.step()

        if scheduler:
            scheduler.step()
        if epoch % config['log_interval']==0:
            batch_time=time.time()-start_time

            x=evaluate_loss(model)
            losses+=[x]
            if print_logs:
                print(f"Epoch {epoch } | val loss {x['val']:.3f} | Time {batch_time:.3f} |ETA in seconds{batch_time*(config['epochs']-epoch)/config['log_interval'] :.3f}")
                start_time=time.time()

                if scheduler:
                    print("lr :",scheduler.get_lr())
        print("validation loss: ",losses[-1]['val'])
    pd.DataFrame(losses).plot()
    plt.show()


class Simple(nn.Module):
    def __init__(self,config):
        super().__init__()
        self.config=config

        self.embedding=nn.Embedding(config['vocab_size'],config['d_model'])
        self.linear=nn.Sequential(
            nn.Linear(config['d_model'],config['d_model']),
            nn.ReLU(),
            nn.Linear(config['d_model'],config['vocab_size'])

        )
        print(f"model params: ",sum([m.numel() for m in self.parameters()]))

    def forward(self,idx,targets=None):
        x=self.embedding(idx)
        logits=self.linear(x)

        if targets is not None:
            loss=F.cross_entropy(logits.view(-1,self.config['vocab_size']),targets.view(-1))
            return logits,loss
        else:
            return logits

model2=Simple(MASTER_CONFIG)
xs,ys=get_batches(dataset,'train',MASTER_CONFIG['batch_size'],MASTER_CONFIG['context_window'])
logits,loss=model2(xs,ys)
optimizer=torch.optim.Adam(model.parameters())





#train(model,optimizer)




train(model2,optimizer)
